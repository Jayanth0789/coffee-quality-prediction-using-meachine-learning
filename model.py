# -*- coding: utf-8 -*-
"""Coffe Quality Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LsE-2-yHkah1SoXPMisvBhLeFP6QVowy
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv('/content/beans_data.csv')

data

data.shape

data.info()

data.describe()

data.isna().sum()

data.duplicated().sum()

data['Color'].value_counts()

df=data.drop(columns=['ID','Number of Bags','Bag Weight','Variety','Processing Method','Overall','Total Cup Points','Moisture Percentage'],axis=1)

numeric_columns = ['Aroma', 'Flavor', 'Aftertaste', 'Acidity', 'Body', 'Balance', 'Uniformity']
df[numeric_columns].hist(bins=20, figsize=(15, 10), layout=(3, 4), color='skyblue', edgecolor='black')
plt.suptitle('Histograms of Coffee Quality Scores', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

plt.figure(figsize=(12,10))
plt.pie(df['Color'].value_counts(), labels=df['Color'].value_counts().index, autopct='%1.1f%%')
plt.title('Ratios of Coffee Bean Colors')
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(df['Flavor'], df['Aftertaste'], alpha=0.7)
plt.xlabel('flavor')
plt.ylabel('aftertaste')
plt.title('Scatter Plot of Flavor and Aftertaste Scores')
plt.show()

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Aroma', y='Flavor', data=df, color='blue', alpha=0.7)
plt.title('Scatter Plot: Aroma vs. Flavor')
plt.xlabel('Aroma')
plt.ylabel('Flavor')
plt.show()

# Select only numerical columns for correlation calculation
numerical_df = df.select_dtypes(include=np.number)

# Calculate and plot the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(numerical_df.corr(), annot=True)
plt.show()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['Color_Encoded'] = label_encoder.fit_transform(df['Color'])
df = df.drop(['Color'], axis=1)

df

df['Bean_Status'] = 'Healthy'
condition_healthy = (df['Category One Defects'] == 0) & (df['Category Two Defects'] == 0)
df.loc[condition_healthy, 'Bean_Status'] = 'Healthy'
condition_unhealthy = (df['Category One Defects'] != 0) & (df['Category Two Defects'] != 0)
df.loc[condition_unhealthy, 'Bean_Status'] = 'Unhealthy'

df

df['Bean_Status'].value_counts()

df['Bean_Status_Encoded']=label_encoder.fit_transform(df['Bean_Status'])
df=df.drop(['Bean_Status'],axis=1)

df

df

plt.figure(figsize=(8, 6))

sns.boxplot(df['Aroma'])

plt.title('Boxplot of Aroma')

plt.show()

quant = df.Aroma.quantile(q=[0.75,0.5,0.25,1])

quant

Q3_Aroma = df['Aroma'].quantile(0.75)

Q3_Aroma

Q1_Aroma = df['Aroma'].quantile(0.25)

Q1_Aroma

IQR_Aroma =Q3_Aroma-Q1_Aroma

IQR_Aroma

Max_Whisker_Aroma = Q3_Aroma + 1.5* IQR_Aroma
Max_Whisker_Aroma

Min_Whisker_Aroma = Q1_Aroma - 1.5 * IQR_Aroma

Min_Whisker_Aroma

df = df[(df['Aroma'] >= Min_Whisker_Aroma) & (df['Aroma'] <= Max_Whisker_Aroma)]

plt.figure(figsize=(8, 6))

sns.boxplot(df['Aroma'])

plt.title('Boxplot of Aroma')

plt.show()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto', random_state=42)

xbal,ybal=smote.fit_resample(x_train,y_train)

ybal.value_counts()

df1=df.drop(['Category One Defects','Category Two Defects'],axis=1)

x=df1.drop(['Bean_Status_Encoded'],axis=1)
y=df1['Bean_Status_Encoded']

x

from sklearn.linear_model import LogisticRegression
LR=LogisticRegression()

LR.fit(xbal,ybal)

y_test_pred=LR.predict(x_test)
y_train_pred=LR.predict(x_train)

from sklearn.metrics import accuracy_score

accuracy_lr=accuracy_score(y_test,y_test_pred)

print(accuracy_score(y_test,y_test_pred))
print(accuracy_score(y_train,y_train_pred))

from sklearn.tree import DecisionTreeClassifier

DTC=DecisionTreeClassifier(criterion='gini')

DTC.fit(xbal,ybal)

y_test_pred1=DTC.predict(x_test)

y_train_pred1=DTC.predict(x_train)

accuracy_dtc = accuracy_score(y_test,y_test_pred1)

print(accuracy_score(y_test, y_test_pred1))

print(accuracy_score(y_train, y_train_pred1))

from sklearn.metrics import classification_report

print(classification_report(y_test,y_test_pred1))

print(classification_report(y_train,y_train_pred1))

from sklearn.ensemble import RandomForestClassifier
RFC=RandomForestClassifier(n_estimators=6)
RFC.fit(xbal,ybal)

y_test_pred2=RFC.predict(x_test)
y_train_pred2=RFC.predict(x_train)

accuracy_score(y_test_pred2, y_test)

accuracy_rfc = accuracy_score(y_test,y_test_pred2)

print(accuracy_score(y_test, y_test_pred2))

print(accuracy_score(y_train, y_train_pred2))

print(classification_report(y_test,y_test_pred2))

print(classification_report(y_train,y_train_pred2))

c_matrix_train_rfc = confusion_matrix(y_train, y_train_pred2)
c_matrix_train_rfc

c_matrix_test_rfc = confusion_matrix(y_test,y_test_pred2)
c_matrix_test_rfc

models=['logistic regression', 'decision tree', 'random forest']
accuracy=[accuracy_lr*100, accuracy_dtc*100, accuracy_rfc*100]
sns.barplot(x=models,y=accuracy, color='Green')
plt.show()

import pickle
pickle.dump(RFC,open("rfc.pkl","wb"))

print(RFC.predict([[1,8,7,415,7.42,159,100,100,3]]))

